{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Problem Set #1 - 4. Naive Bayes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXw4-qzxMsgR",
        "colab_type": "text"
      },
      "source": [
        "### CS229 Problem Set #1\n",
        "\n",
        "# CS 229, Public Course\n",
        "\n",
        "## Problem Set #1: Supervised Learning\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mmxGWOgMsgT",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes\n",
        "\n",
        "In this problem, we look at **maximum likelihood parameter estimation** using the [***naive Bayes***](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) assumption.\n",
        "\n",
        "Here, the input features $x_j$ , $j = 1, . . . , n$ to our model are ***discrete, binary-valued variables***, so each\n",
        "\n",
        ">$$x_j \\in \\{0, 1\\}$$\n",
        "\n",
        "We call $x = [x_1 x_2 \\dots x_n]^T$ to be the input vector.\n",
        "\n",
        "For each training example, our **output** targets are a ***single binary-value***\n",
        "\n",
        ">$$y \\in \\{0, 1\\}$$\n",
        "\n",
        "Our **model is then parameterized by**\n",
        "\n",
        ">$$\\phi_{j|y=0} = p(x_j = 1|y = 0)$$\n",
        ">\n",
        ">$$\\phi_{j|y=1} = p(x_j = 1|y = 1)$$\n",
        ">\n",
        ">and\n",
        ">\n",
        ">$$\\phi_{y} = p(y = 1)$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9gsJSLCQqkk",
        "colab_type": "text"
      },
      "source": [
        "We model the **joint** distribution of $(x, y)$ according to\n",
        "\n",
        ">|$$p(y) =$$| |\n",
        ">|-|-|\n",
        ">||$$ = (\\phi_y)^y(1 - \\phi_y)^{1-y}$$|\n",
        "\n",
        "\n",
        ">|$$p(x | y=0) =$$| |\n",
        ">|-|-|\n",
        ">||$$ = \\prod_{j=1}^n p(x_j | y=0)$$|\n",
        ">||$$= \\prod_{j=1}^n (\\phi_{j|y=0})^{x_j} (1- \\phi_{j|y=0})^{1 - x_j}$$|\n",
        "\n",
        ">|$$p(x | y=1) =$$| |\n",
        ">|-|-|\n",
        ">||$$ = \\prod_{j=1}^n p(x_j | y=1)$$|\n",
        ">||$$= \\prod_{j=1}^n (\\phi_{j|y=1})^{x_j} (1- \\phi_{j|y=1})^{1 - x_j}$$|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OI6fFQMSFS6",
        "colab_type": "text"
      },
      "source": [
        "* **a)** Find the **joint likelihood function**\n",
        "\n",
        "$$\\ell ( \\varphi) = log \\prod_{i=1}^m p(x^{(i)}, y^{(i)}; \\varphi)$$ in terms of the model parameters given above.\n",
        "\n",
        "Here, $\\varphi$ represents the entire **set of parameters**:\n",
        "\n",
        ">$\\phi_y$\n",
        ">\n",
        ">$\\phi_{j|y=0}, $\n",
        ">\n",
        ">$\\phi_{j|y=1},$\n",
        ">\n",
        ">$j = 1, \\dots , n$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8hMXGMwT2Va",
        "colab_type": "text"
      },
      "source": [
        "* **b)** Show that the parameters which maximize the likelihood function are the same as those given in the lecture notes; i.e., that\n",
        "\n",
        ">$$\\phi_{j|y=0} = \\frac {\\sum_{i=1}^m \\mathbf 1 \\{x_j^{(i)} = 1 \\land y^{(i)} = 0\\}}\n",
        "{\\sum_{i=1}^m \\mathbf 1 \\{ y^{(i)} = 0\\} }$$\n",
        ">\n",
        "\n",
        ">$$\\phi_{j|y=1} = \\frac {\\sum_{i=1}^m \\mathbf 1 \\{x_j^{(i)} = 1 \\land y^{(i)} = 1\\}}\n",
        "{\\sum_{i=1}^m \\mathbf 1 \\{ y^{(i)} = 1\\} }$$\n",
        ">\n",
        "\n",
        ">$$\\phi_y = \\frac {\\sum_{i=1}^m \\mathbf 1 \\{y^{(i)} = 1\\}}\n",
        "{m}$$\n",
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6af0JKNVM2k",
        "colab_type": "text"
      },
      "source": [
        "* c) Consider making a prediction on some **new data point** $x$ using the most likely class estimate generated by the ***naive Bayes algorithm***.\n",
        "\n",
        "Show that the **hypothesis** returned by naive Bayes is a **linear classifier**, i.e.,\n",
        "\n",
        "if \n",
        ">$p(y = 0|x)$\n",
        ">\n",
        "\n",
        "and \n",
        ">\n",
        ">$p(y = 1|x)$\n",
        "\n",
        "are the class probabilities returned by naive Bayes, show that:\n",
        "\n",
        ">there exists some $\\theta \\in R^{n+1}$ such that\n",
        ">\n",
        ">$$p(y = 1|x) â‰¥ p(y = 0|x) \\Longleftrightarrow \\theta^T\n",
        "\\begin{bmatrix}1\\\\\n",
        "x \\end{bmatrix}\n",
        "\\geq 0$$\n",
        ">\n",
        ">(Assume $\\theta_0$ is an intercept term.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNGaIwWWT5Fq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2rp3YSLT5QI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuUN-ki_T5eT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKU76CSCT57t",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mndaUanlT6Er",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiCx3XKOT6OM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOWN8IrWT6XO",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9baDTI3oT6hN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMLUBUJtT6rT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwaFL7WIT60l",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-9UX_POT691",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Lt7SwJT5cM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtzXIrcqT5Z0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0YKEWKkT5Xs",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nCBwP3HT5Vl",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4oK3Wa6T5Nu",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk5AREvnT5LC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZKuBOMhT5JS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6bpu8aET5DC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkb8hHFnMxrR",
        "colab_type": "text"
      },
      "source": [
        ">$$\\{(x^{(i)}, \\  y^{(i)})\\ , \\ i=1,\\dots,m\\}$$\n",
        ">\n",
        ">$$x^{(i)} \\in \\mathbb R ^n$$\n",
        ">$$y^{(i)} \\in \\mathbb R ^p$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MjTaydPNcde",
        "colab_type": "text"
      },
      "source": [
        "Thus for each training example, $y^{(i)}$ is ***vector-valued***, with $p$ **entries**.\n",
        "\n",
        "We wish to use a **linear model** to predict the outputs, as in least squares, by specifying the parameter matrix $\\Theta$ in\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuGY1IMuNw12",
        "colab_type": "text"
      },
      "source": [
        "> $$y = \\Theta ^T x$$\n",
        ">\n",
        "> where $\\Theta \\in \\mathbb R ^ {n \\times p}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABkmQrFrNcr_",
        "colab_type": "text"
      },
      "source": [
        "* **a)** The cost function for this case is\n",
        "  \n",
        "  >  $$J(\\Theta) = \\frac 1 2 \\sum_{i=1}^m \\sum_{j=1}^p \\left( (\\Theta^T x^{(i)})_j - y_j ^{(i)}\\right)^2$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQxkFJv1Ncwv",
        "colab_type": "text"
      },
      "source": [
        "  Write $J(\\Theta)$ in **matrix-vector notation** (i.e., without using any summations).\n",
        "  \n",
        "  > ***Hint:*** Start with the $m \\times n$ ***design matrix***:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRseGCzBPwv8",
        "colab_type": "text"
      },
      "source": [
        "$$X = \\begin{bmatrix}\n",
        "- \\ \\ (x^{(1)})^T \\ \\ - \\\\\n",
        "- \\ \\ (x^{(2)})^T \\ \\ - \\\\\n",
        "\\vdots \\\\\n",
        "- \\ \\ (x^{(m)})^T \\ \\ - \\\\\n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_a0STmCNczf",
        "colab_type": "text"
      },
      "source": [
        ">  and the $m \\times p$ ***target matrix***:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyNV6l33RkKv",
        "colab_type": "text"
      },
      "source": [
        "$$Y = \\begin{bmatrix}\n",
        "- \\ \\ (y^{(1)})^T \\ \\ - \\\\\n",
        "- \\ \\ (y^{(2)})^T \\ \\ - \\\\\n",
        "\\vdots \\\\\n",
        "- \\ \\ (y^{(m)})^T \\ \\ - \\\\\n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOCfKvqmNcuk",
        "colab_type": "text"
      },
      "source": [
        "> and then work out how to express $J(\\Theta)$ in terms of these matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug0IGhICT5B-",
        "colab_type": "text"
      },
      "source": [
        "### Solution:\n",
        "| Cost function|\n",
        "|--|\n",
        "|$$\\huge J(\\Theta) = \\frac 1 2 (X \\Theta - y)^T(X \\Theta - y)$$|\n",
        "\n",
        "> where\n",
        ">\n",
        ">|Matrix|$$X$$|$$\\Theta$$|$$y$$|\n",
        ">|--|--|--|--|\n",
        ">|Dim|$$m \\times n$$|$$n \\times p$$|$$m \\times p$$|\n",
        ">\n",
        ">\n",
        ">| |$$(X \\Theta - y)^T$$|$$X \\Theta - y$$|\n",
        ">|--|--|--|\n",
        ">|Dim|$$p \\times m$$|$$m \\times p$$|\n",
        ">\n",
        ">\n",
        ">| |$$J(\\Theta)$$|\n",
        ">|--|--|\n",
        ">|Dim|$$p \\times p$$|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R5xZaOVR0Vm",
        "colab_type": "text"
      },
      "source": [
        "* **b)** Find the **closed form solution** for $\\Theta$ which minimizes $J(\\Theta)$.\n",
        "  \n",
        "  This is the **equivalent** to the ***normal equations*** for the **multivariate case**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i2WDOhkbvd-",
        "colab_type": "text"
      },
      "source": [
        "### Solution:\n",
        "\n",
        "From part 1.\n",
        "\n",
        "> |Gradient of loss|\n",
        "> |-|\n",
        "> |$$\\large \\nabla_\\theta J(\\theta) = X^T \\left( X \\theta - y \\right)$$|\n",
        "\n",
        "Expanding:\n",
        "\n",
        "$$\\nabla_\\theta J(\\Theta) = X^T X \\theta - X^T y$$\n",
        "\n",
        "Want to minimize loss\n",
        "\n",
        "$$\\nabla_\\Theta J(\\Theta) \\stackrel{\\text{set}}{=} 0$$\n",
        "\n",
        "\n",
        "$$X^T X \\Theta - X^T y = 0$$\n",
        "\n",
        "\n",
        "$$X^T X \\Theta = X^T y$$\n",
        "\n",
        "> |Closed form solution for $\\theta$|\n",
        "> |-|\n",
        "|$$\\huge \\Theta = (X^T X)^{-1} X^T y$$|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtluOLE8R0ag",
        "colab_type": "text"
      },
      "source": [
        "* **c)** Suppose **instead** of considering the multivariate vectors $y^{(i)}$ **all at once**, we **instead compute each variable** $y_j^{(i)}$ separately for each $j = 1, \\dots , p$.\n",
        "\n",
        "  In this case, we have $p$ individual linear models, of the form:\n",
        "\n",
        "  > $$y_j^{(i)} = \\theta_j^T x^{(i)}$$\n",
        "  >\n",
        "  > with $j = 1, \\dots, p$\n",
        "\n",
        "  So here, each $\\theta_j \\in \\mathbb R ^n$.\n",
        "  \n",
        "  How do the parameters from these $p$ **independent least squares** problems **compare** to the **multivariate** solution?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik5exXV7dohA",
        "colab_type": "text"
      },
      "source": [
        "### Solution:\n",
        "\n",
        "Each $\\theta_j$ corresponds to an **output dimension** $j$, where $j = 1,\\dots, p$\n",
        "\n",
        "$$\\Theta = \\begin{bmatrix}\n",
        "- \\theta_1 - \\\\\n",
        "- \\theta_2 - \\\\\n",
        " \\vdots \\\\\n",
        "- \\theta_p - \\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Where each\n",
        "$$\\theta_j = (X^T X)^{-1} X^T y_j$$\n",
        "\n",
        "Concretely\n",
        "\n",
        "$$\\Theta = \\begin{bmatrix}\n",
        "(X^T X)^{-1} X^T y_1 \\\\\n",
        "(X^T X)^{-1} X^T y_2 \\\\\n",
        " \\vdots \\\\\n",
        "(X^T X)^{-1} X^T y_p \\\\\n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCCxAJWzSpv4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2_6WEenR0di",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0NLTOeyR0ij",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "439eZHvNR0gl",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNHCF2L1R0TG",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhA47JptMsgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}