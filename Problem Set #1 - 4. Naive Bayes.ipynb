{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Problem Set #1 - 4. Naive Bayes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXw4-qzxMsgR",
        "colab_type": "text"
      },
      "source": [
        "### CS229 Problem Set #1\n",
        "\n",
        "# CS 229, Public Course\n",
        "\n",
        "## Problem Set #1: Supervised Learning\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mmxGWOgMsgT",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes\n",
        "\n",
        "In this problem, we look at **maximum likelihood parameter estimation** using the [***naive Bayes***](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) assumption.\n",
        "\n",
        "Here, the input features $x_j$ , $j = 1, . . . , n$ to our model are ***discrete, binary-valued variables***, so each\n",
        "\n",
        ">$$x_j \\in \\{0, 1\\}$$\n",
        "\n",
        "We call $x = [x_1 x_2 \\dots x_n]^T$ to be the input vector.\n",
        "\n",
        "For each training example, our **output** targets are a ***single binary-value***\n",
        "\n",
        ">$$y \\in \\{0, 1\\}$$\n",
        "\n",
        "Our **model is then parameterized by**\n",
        "\n",
        ">$$\\phi_{j|y=0} = p(x_j = 1|y = 0)$$\n",
        ">\n",
        ">$$\\phi_{j|y=1} = p(x_j = 1|y = 1)$$\n",
        ">\n",
        ">and\n",
        ">\n",
        ">$$\\phi_{y} = p(y = 1)$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9gsJSLCQqkk",
        "colab_type": "text"
      },
      "source": [
        "We model the **joint** distribution of $(x, y)$ according to\n",
        "\n",
        ">|$$p(y) =$$| |\n",
        ">|-|-|\n",
        ">||$$ = (\\phi_y)^y(1 - \\phi_y)^{1-y}$$|\n",
        "\n",
        "\n",
        ">|$$p(x | y=0) =$$| |\n",
        ">|-|-|\n",
        ">||$$ = \\prod_{j=1}^n p(x_j | y=0)$$|\n",
        ">||$$= \\prod_{j=1}^n (\\phi_{j|y=0})^{x_j} (1- \\phi_{j|y=0})^{1 - x_j}$$|\n",
        "\n",
        ">|$$p(x | y=1) =$$| |\n",
        ">|-|-|\n",
        ">||$$ = \\prod_{j=1}^n p(x_j | y=1)$$|\n",
        ">||$$= \\prod_{j=1}^n (\\phi_{j|y=1})^{x_j} (1- \\phi_{j|y=1})^{1 - x_j}$$|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OI6fFQMSFS6",
        "colab_type": "text"
      },
      "source": [
        "* **a)** Find the **joint likelihood function**\n",
        "\n",
        "$$\\ell ( \\varphi) = log \\prod_{i=1}^m p(x^{(i)}, y^{(i)}; \\varphi)$$ in terms of the model parameters given above.\n",
        "\n",
        "Here, $\\varphi$ represents the entire **set of parameters**:\n",
        "\n",
        ">$\\phi_y$\n",
        ">\n",
        ">$\\phi_{j|y=0}, $\n",
        ">\n",
        ">$\\phi_{j|y=1},$\n",
        ">\n",
        ">$j = 1, \\dots , n$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Sw--wrEmZ7n",
        "colab_type": "text"
      },
      "source": [
        "# Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glJV3XzqmhfL",
        "colab_type": "text"
      },
      "source": [
        ">$$\\ell ( \\varphi) = log \\prod_{i=1}^m p(x^{(i)}, y^{(i)}; \\varphi)$$\n",
        "\n",
        ">$p(x,y) = p(x|y)p(y)$\n",
        ">\n",
        ">$$= log \\prod_{i=1}^m p(x^{(i)}| y^{(i)}; \\varphi) p(y^{(i)}; \\varphi)$$\n",
        "\n",
        ">from now on\n",
        ">\n",
        ">$$= log \\prod_{i=1}^m p(x^{(i)}| y^{(i)}) p(y^{(i)})$$\n",
        "\n",
        "> Because **Naive Bayes assumption**:\n",
        "> \n",
        "> * \"*Conditional probabilities are independent*\": All $x^{(i)}$ dimensions are independent\n",
        ">\n",
        "> $$p(x^{(i)}| y^{(i)})  = \\prod_{j=1}^n p(x_j^{(i)}| y^{(i)})$$\n",
        "\n",
        "> So\n",
        "> $$log \\prod_{i=1}^m p(x^{(i)}| y^{(i)}) p(y^{(i)}) =$$\n",
        ">\n",
        "> Naive Bayes assumption\n",
        ">\n",
        ">$$ = log \\left( \\prod_{i=1}^m \\left( \\prod_{j=1}^n p(x_j^{(i)}| y^{(i)}) \\right) p(y^{(i)}) \\right)$$\n",
        "> \n",
        "> log of product $\\equiv$ sum of log\n",
        "> $$= \\sum_{i=1}^m log \\left(\\left( \\prod_{j=1}^n p(x_j^{(i)}| y^{(i)}) \\right) p(y^{(i)}) \\right)$$\n",
        ">\n",
        "> $$= \\sum_{i=1}^m \\left( log \\left( \\prod_{j=1}^n p(x_j^{(i)}| y^{(i)}) \\right) + log \\ p(y^{(i)}) \\right)$$\n",
        ">\n",
        "> To get rid of parenthesis\n",
        "> $$= \\sum_{i=1}^m \\left( log \\ p(y^{(i)}) + log \\prod_{j=1}^n p(x_j^{(i)}| y^{(i)}) \\right)$$\n",
        ">\n",
        "> $$\\ell ( \\varphi) = \\sum_{i=1}^m \\left( log \\ p(y^{(i)}) + \\sum_{j=1}^n log \\ p(x_j^{(i)}| y^{(i)}) \\right)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdhPZAOgqsWN",
        "colab_type": "text"
      },
      "source": [
        "We need the expressions for\n",
        "* $log \\ p(x_j^{(i)}| y^{(i)})  $\n",
        "* $ log \\ p(y^{(i)})$\n",
        "\n",
        "First, the shorter one:\n",
        "* $ log \\ p(y^{(i)})$\n",
        "> $p(y^{(i)})$ was given as $(\\phi _y)^y \\ (1-\\phi_y)^{(1-y)}$\n",
        ">\n",
        "> because $y \\sim Bernoulli(\\phi_y)$\n",
        ">\n",
        "> So\n",
        ">\n",
        "> $$ log \\left( p(y^{(i)}) \\right) = log \\left( (\\phi _y)^y \\ (1-\\phi_y)^{(1-y)}\\right)$$\n",
        ">\n",
        ">$$= y \\ log \\left( \\phi _y \\right) + (1-y) \\ log \\left( 1-\\phi_y \\right)$$\n",
        "\n",
        "Now, the other term:\n",
        "* $log \\ p(x_j^{(i)}| y^{(i)})$\n",
        "> Is given that\n",
        "> $$p(x_j^{(i)}| y^{(i)}) = (\\phi _{j|y})^{x_j^{(i)}} \\ (1-\\phi_{j|y})^{(1-x_j^{(i)})}$$\n",
        ">\n",
        "> So\n",
        ">\n",
        "> $$log \\ p(x_j^{(i)}| y^{(i)}) = log \\left( (\\phi _{j|y})^{x_j^{(i)}} \\ (1-\\phi_{j|y})^{(1-{x_j^{(i)}})}\\right)$$\n",
        ">\n",
        "> $$= x_j^{(i)} log  (\\phi _{j|y}) + (1-x_j^{(i)}) log (1-\\phi_{j|y})$$\n",
        ">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXBZghCHzZE_",
        "colab_type": "text"
      },
      "source": [
        "Going back to\n",
        "\n",
        "> $$\\ell ( \\varphi) = \\sum_{i=1}^m \\left( log \\ p(y^{(i)}) + \\sum_{j=1}^n log \\ p(x_j^{(i)}| y^{(i)}) \\right)$$\n",
        "\n",
        "> | **Solution**|\n",
        "> |--|\n",
        "> |$$\\large \\ell ( \\varphi) = \\sum_{i=1}^m \\left( \\left[ y \\ log \\left( \\phi _y \\right) + (1-y) \\ log \\left( 1-\\phi_y \\right)\\right] +  \\sum_{j=1}^n \\left[ x_j^{(i)} log  (\\phi _{j|y}) + (1-x_j^{(i)}) log (1-\\phi_{j|y}) \\right] \\right) $$|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLNA0AEgmfPP",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8hMXGMwT2Va",
        "colab_type": "text"
      },
      "source": [
        "* **b)** Show that the parameters which maximize the likelihood function are the same as those given in the lecture notes; i.e., that\n",
        "\n",
        ">$$\\phi_{j|y=0} = \\frac {\\sum_{i=1}^m \\mathbf 1 \\{x_j^{(i)} = 1 \\land y^{(i)} = 0\\}}\n",
        "{\\sum_{i=1}^m \\mathbf 1 \\{ y^{(i)} = 0\\} }$$\n",
        ">\n",
        "\n",
        ">$$\\phi_{j|y=1} = \\frac {\\sum_{i=1}^m \\mathbf 1 \\{x_j^{(i)} = 1 \\land y^{(i)} = 1\\}}\n",
        "{\\sum_{i=1}^m \\mathbf 1 \\{ y^{(i)} = 1\\} }$$\n",
        ">\n",
        "\n",
        ">$$\\phi_y = \\frac {\\sum_{i=1}^m \\mathbf 1 \\{y^{(i)} = 1\\}}\n",
        "{m}$$\n",
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Yw0v_93Ts3",
        "colab_type": "text"
      },
      "source": [
        "if\n",
        "> $$ \\ell ( \\varphi) = \\sum_{i=1}^m \\left( \\left[ y \\ log \\left( \\phi _y \\right) + (1-y) \\ log \\left( 1-\\phi_y \\right)\\right] +  \\sum_{j=1}^n \\left[ x_j^{(i)} log  (\\phi _{j|y}) + (1-x_j^{(i)}) log (1-\\phi_{j|y}) \\right] \\right) $$\n",
        "\n",
        "then\n",
        "> $$\\nabla _{\\phi_{j|y=0}}  \\ell (\\varphi) = \\sum_{i=1}^m \\left( 0 + \\nabla _{\\phi_{j|y=0}}  \\left[ x_j^{(i)} log  (\\phi _{j|y^{(i)}}) + (1-x_j^{(i)}) log (1-\\phi_{j|y^{(i)}}) \\right] \\right) $$\n",
        ">\n",
        ">$$= \\sum_{i=1}^m \\nabla _{\\phi_{j|y=0}}   \\left[ x_j^{(i)} log  (\\phi _{j|y^{(i)}}) + (1-x_j^{(i)}) log (1-\\phi_{j|y^{(i)}}) \\right]  $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBZJP2kH_Z6p",
        "colab_type": "text"
      },
      "source": [
        "If we are deriving wrt $\\phi_{j|y=0}$,\n",
        "\n",
        "the only terms that will **not become zero**\n",
        "\n",
        "will be the examples $(i)$ with $y^{(i)} $\n",
        "\n",
        "> Observe:\n",
        ">\n",
        "> \\begin{cases} \n",
        "      \\nabla _{\\phi_{j|y=0}} log  (\\phi _{j|y^{(i)}}) = \\frac 1 {  \\phi _{j|y^{(i)}=0}} & \\text{if} \\ \\ y^{(i)} = 0 \\\\\n",
        "      \\nabla _{\\phi_{j|y=0}} log  (\\phi _{j|y^{(i)}}) = 0 & \\text{if} \\ \\ y^{(i)} = 1 \n",
        "   \\end{cases}\n",
        "\n",
        "So we only care about $y^{(i)}=0$ for this particular partial derivative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EELVDgvCM-n",
        "colab_type": "text"
      },
      "source": [
        "$$\\nabla _{\\phi_{j|y=0}} log  (\\phi _{j|y^{(i)}}) = \\nabla _{\\phi_{j|y=0}} log  (\\phi _{j|y^{(i)}=0}) \\ \\mathbf 1\\{y^{(i)}=0\\}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIsiyCwi5ET9",
        "colab_type": "text"
      },
      "source": [
        "> expand and only remain terms with $\\phi_{j|y=0}$\n",
        ">$$= \\sum_{i=1}^m \\nabla _{\\phi_{j|y=0}}  \\left[ x_j^{(i)} log  (\\phi _{j|y=0}) \\mathbf 1\\{y^{(i)}=0\\} + (1-x_j^{(i)}) log (1-\\phi_{j|y=0}) \\mathbf 1\\{y^{(i)}=0\\} \\right]  $$\n",
        ">\n",
        ">$$\\nabla _{\\phi_{j|y=0}}  \\ell (\\varphi) = \\sum_{i=1}^m  \\left[ x_j^{(i)} \\frac 1 {\\phi _{j|y=0}} \\mathbf 1\\{y^{(i)}=0\\} + (1-x_j^{(i)}) \\frac 1 {1-\\phi_{j|y=0}} \\right] \\mathbf 1\\{y^{(i)}=0\\} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UideMu7aCu6s",
        "colab_type": "text"
      },
      "source": [
        "* Only rests set derivative to zero and solve for $\\phi _{j|y=0}$\n",
        "\n",
        "* Same procedure for $\\phi _{j|y=1}$ and $\\phi_y$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bo-VFAuDKs6",
        "colab_type": "text"
      },
      "source": [
        "> > For more details, check **Andrew Ng's solutions**:\n",
        ">\n",
        "> web: https://see.stanford.edu/Course/CS229/47\n",
        ">\n",
        "> local: ./andrews solutions/only open if you tried at least 3 long times/ps1_solution.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9UmPnljD634",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6af0JKNVM2k",
        "colab_type": "text"
      },
      "source": [
        "* c) Consider making a prediction on some **new data point** $x$ using the most likely class estimate generated by the ***naive Bayes algorithm***.\n",
        "\n",
        "Show that the **hypothesis** returned by naive Bayes is a **linear classifier**, i.e.,\n",
        "\n",
        "if \n",
        ">$p(y = 0|x)$\n",
        ">\n",
        "\n",
        "and \n",
        ">\n",
        ">$p(y = 1|x)$\n",
        "\n",
        "are the class probabilities returned by naive Bayes, show that:\n",
        "\n",
        ">there exists some $\\theta \\in R^{n+1}$ such that\n",
        ">\n",
        ">$$p(y = 1|x) â‰¥ p(y = 0|x) \\Longleftrightarrow \\theta^T\n",
        "\\begin{bmatrix}1\\\\\n",
        "x \\end{bmatrix}\n",
        "\\geq 0$$\n",
        ">\n",
        ">(Assume $\\theta_0$ is an intercept term.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNGaIwWWT5Fq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2rp3YSLT5QI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuUN-ki_T5eT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKU76CSCT57t",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mndaUanlT6Er",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiCx3XKOT6OM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOWN8IrWT6XO",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9baDTI3oT6hN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMLUBUJtT6rT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwaFL7WIT60l",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-9UX_POT691",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Lt7SwJT5cM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtzXIrcqT5Z0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0YKEWKkT5Xs",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nCBwP3HT5Vl",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4oK3Wa6T5Nu",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk5AREvnT5LC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZKuBOMhT5JS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6bpu8aET5DC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}